# Список используемой литературы

- **Машинный перевод**  
  [https://yandex.ru/company/technologies/translation/](https://yandex.ru/company/technologies/translation/)  
  Дата доступа: 15.03.2025

- **Google Translate**  
  [https://en.wikipedia.org/wiki/Google_Translate](https://en.wikipedia.org/wiki/Google_Translate)  
  Дата доступа: 15.03.2025

- **How does DeepL work?**  
  [https://www.deepl.com/en/blog/how-does-deepl-work](https://www.deepl.com/en/blog/how-does-deepl-work)  
  Дата доступа: 15.03.2025

- **Attention Is All You Need**  
  [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)  
  Дата доступа: 15.03.2025

- **RNN vs GRU vs LSTM**  
  [https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573](https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573)  
  Дата доступа: 15.03.2025

- **Метод обратного распространения ошибки**  
  [https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki](https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki)  
  Дата доступа: 15.03.2025

- **Understanding the T5 Model: A Comprehensive Guide**  
  [https://medium.com/@gagangupta_82781/understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b](https://medium.com/@gagangupta_82781/understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b)  
  Дата доступа: 20.03.2025

- **BERT Explained: State of the art language model for NLP**  
  [https://medium.com/data-science/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://medium.com/data-science/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)  
  Дата доступа: 20.03.2025

- **An Overview of the Various BERT Pre-Training Methods**  
  [https://medium.com/analytics-vidhya/an-overview-of-the-various-bert-pre-training-methods-c365512342d8](https://medium.com/analytics-vidhya/an-overview-of-the-various-bert-pre-training-methods-c365512342d8)  
  Дата доступа: 20.03.2025

- **NSP-BERT: A Prompt-based Few-Shot Learner Through an Original Pre-training Task —— Next Sentence Prediction**  
  [https://arxiv.org/pdf/2109.03564](https://arxiv.org/pdf/2109.03564)  
  Дата доступа: 20.03.2025

- **European Parliament Proceedings Parallel Corpus 1996-2011**  
  [https://www.statmt.org/europarl](https://www.statmt.org/europarl)  
  Дата доступа: 20.03.2025

- **Translation**  
  [https://huggingface.co/docs/transformers/en/tasks/translation](https://huggingface.co/docs/transformers/en/tasks/translation)  
  Дата доступа: 20.03.2025

- **ELI5-Category**  
  [https://huggingface.co/datasets/rexarski/eli5_category](https://huggingface.co/datasets/rexarski/eli5_category)  
  Дата доступа: 20.03.2025

- **Masked language modeling**  
  [https://huggingface.co/docs/transformers/en/tasks/masked_language_modeling](https://huggingface.co/docs/transformers/en/tasks/masked_language_modeling)  
  Дата доступа: 20.03.2025
